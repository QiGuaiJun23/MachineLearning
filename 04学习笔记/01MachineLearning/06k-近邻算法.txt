分类算法-K-近邻算法（KNN）（最简单）

定义：如果一个样本在特征空间中的k个最相似（即特征空间中最近邻）的样本中的大多数属于某一个类别，则该样本也属于这个类别。

来源：KNN算法最早由Cover和Hart提出的一种分类算法

如何求距离：欧式距离

相似的样本，特征之间的值应该是相近的

k-近邻算法：需要做标准化

sklearn k-近邻算法API

	sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')

		n_neighbors：int,可选（默认=5），k_neighbors查询默认使用的邻居数

		algorithm:{'auto','ball_tree','kd_tree','brute'},可选用于计算最近邻居的算法：'ball_tree'将会使用BallTree,'kd_tree'将使用KDTree.'auto'将尝试根据传递给fit方法的值来决定最适合的算法。（不同实现方式影响效率 ）


实例分析：

特征值：x,y坐标，定位准确性，时间    目标值：入住位置的id

处理：  0<x<10    0<y<10

1、由于数据量大，节省时间x,y缩小

2、时间戳进行（年、月、日、周、时分秒），当做新的特征

3、入住类别（几千~几万），少于指定签到人数的位置删除



k值的影响：
	
	k值取很小：容易受异常点影响

	k值取很大：容易受k值（类别）波动

K-近邻算法优缺点：

	优点：

		简单，易于理解，易于实现，无需估计参数（算法里面），无需训练（无需迭代）

	缺点：

		懒惰算法，对测试样本分类时的计算量大，内存开销大

		必须指定k值，k值选择不当则分类精度不能保证

	使用场景：小数据场景，几千~几万样本，具体场景具体业务去测试