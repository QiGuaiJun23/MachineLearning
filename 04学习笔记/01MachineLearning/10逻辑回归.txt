sklearn模型的保存和加载

from sklearn.externals import joblib

保存和加载API

	保存：

		joblib.dump(rf,'test.pkl')

	加载：

		estimator=joblib.load('test.pkl')

	注：文件格式pkl



逻辑回归：线性回归的式子作为的输入     二分类问题     也能得出概率值

应用场景：

	广告点击率

	是否为垃圾邮件

	是否患病

	金融诈骗

	虚假账号

逻辑回归问题是解决二分类问题的利器

线性回归的输入--------sigmoid------>分类        概率值

公式  g(z)=1/(1+e-z)

输出：[0,1]区间的概率值，默认0.5作为阀值

注：g(z)为sigmoid函数

逻辑回归的损失函数、优化：

	与线性回归原理相同，但由于是分类问题，损失函数不一样，只能通过梯度下降求解

	对数似然损失函数


	cost损失的值越小，那么预测的类别准确度越高


	损失函数：

		均方误差（不存在多个局部最低点）   只有一个最小值

		对数似然损失：多个局部最小值

		1、多次随机初始化，多次比较最小值结果

		2、求解过程当中，调整学习率

		尽量完善，尽管没有全局最低点，但是效果都是不错的

sklearn逻辑回归API:

	sklearn.linear_model.LogisticRegression(penalty='l2',C=1.0)

		Logistic：回归分类器

		coef_:回归系数

哪一个类别少，判定概率值是指的这个类别   举例乳腺癌良性多则良性为正例，恶性多则恶性为反例

LogisticRegression总结：

	优点：适合需要得到一个分类概率的场景，简单，速度快

	缺点：不好处理多分类问题


	      	 				生成模型和判别模型

	      	 判别模型                                生成模型（先验概率）

	          逻辑回归                               朴素贝叶斯

解决问题        二分类                                多分类

应用场景     癌症、二分类需要概率                      文本分类

参数            正则化力度                            没有

						得出的结果都有概率解释


判断模型：k-近邻、决策树、随机森林、神经网络、逻辑回归   

生成模型：朴素贝叶斯、隐马尔科夫模型

判别依据：有没有先验概率（是否需要从历史数据总结出概率信息）