非监督学习 主要方法是：K-means  聚类

K：把数据划分成多少个类别

知道类别的个数
		
不知道类别的个数  	超参数

1、随机在数据当中抽取三个样本，当做三个类别的中心点

2、计算其余的点分别到这三个中心点距离，每一个样本有三个距离(a,b,c)，从中选出距离最近的一个点作为自己的标记，形成三个族群

3、分别计算这三个族群的平均值，把三个平均值与之前的三个旧中心点进行比较，如果相同：结束聚类，如果不相同：把这三个平均值当做新的中心点，重复第二步。


距离的度量：常用欧几里得距离和余弦相似度（先标准化）






K-means步骤：

	1、随机设置K个特征空间内的点作为初始的聚类中心

	2、对于其他每个点计算到k个中心点的距离，未知的点选择最近的一个聚类中心点作为标记类别

	3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）

	4、如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程

K-means API:

	sklearn.cluster.KMeans(n_clusters=8,init='k-means++')

	n_clusters:开始的聚类中心数量

	init:初始化方法，默认为'k-means++'

	labels_:默认标记的类别，可以和真实值比较（不是值比较）


Kmeans性能评估指标

	轮廓系数：

		计算公式：sci=(bi-ai)/max(bi,ai)

	注：对于每个点i为已聚类数据中的样本，bi为i到其他族群的所有样本的距离最小值，ai为i到本身簇的距离平均值

	最终计算出所有的样本点的轮廓系数平均值

	轮廓系数API:

		sklearn.metric.sihouette_score(X,labels)

		计算所有样本的平均轮廓系数

		X：特征值

		labels:被聚类标记的目标值


Kmeans总结：

	特点分析：采用迭代式算法，直观易懂并且非常实用

	缺点：容易收敛到局部最优解（多次聚类）

	注意：聚类一般做在分类之前


优点：
	
	简单，快速，适合常规数据集

缺点：

	k值难确定

	复杂度与样本呈线性关系

	很难发现任意形状的簇




DBSCAN算法：聚类的一种，比kmeans算法还要强大

核心对象：若某个点的密度达到算法设定的阈值则其为核心点(即r领域内点的数量不小于minPts)

领域的距离阈值：设定的半径r

直接密度可达：若某点p在点q的r领域内，且q是核心点则p-q直接密度可达

密度可达：若有一个点的序列q0,q1,...,qk，对任意qi-qi-1是直接密度可达的，则称从q0到qk密度可达，这实际上是直接密度可达的"传播"


API:from sklearn.cluster import DBSCAN

DBSCAN算法适合检测异常点和离群点

参数选择：

	半径：可以根据K距离来设定：找突变点

	K距离：给定数据集P={p(i),i=0,1,2,...,n},计算点P(i)到集合D的子集S中所有点之间的距离，距离按照从小到大的顺序排序，d(k)就被称为K-距离

	MinPts:K-距离中k的值，一般取的小一些，多次尝试

优点：

	不需要指定簇的个数

	可以发现任意形状的簇

	擅长找到离群点（检测任务）

	两个参数就够了

缺点：

	高维数据有些困难（可以做降维）

	参数难以选择（参数对结果的影响非常大）

	Sklearn中效率很慢（数据削减策略）

